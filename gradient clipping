import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras.utils import to_categorical

# Step 1: Load and preprocess the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# One-hot encode the target labels
y = to_categorical(y)

# Split the dataset into training and validation sets (70% train, 30% validation)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Normalize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Step 2: Build the neural network model
model = Sequential()

# Input layer
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))

# Add dropout layer for regularization
model.add(Dropout(0.2))

# Hidden layer
model.add(Dense(32, activation='relu'))

# Add another dropout layer for regularization
model.add(Dropout(0.3))

# Output layer
model.add(Dense(y_train.shape[1], activation='softmax'))  # Softmax for multi-class classification

# Step 3: Compile the model with gradient clipping
optimizer = Adam(clipvalue=1.0)  # Clip gradients with a value of 1.0 (maximum gradient size)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Step 4: Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_val, y_val))

# Step 5: Plot the training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss with Gradient Clipping')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
